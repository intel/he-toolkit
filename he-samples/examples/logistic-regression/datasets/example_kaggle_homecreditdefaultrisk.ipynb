{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and saving real world data set\n",
    "This is a sample code to train a logistic regression model, then store the dataset and model for LR HE inference example with a real world dataset. It has been tested on ```python3 >= 3.5```.\n",
    "\n",
    "### Requirements\n",
    "```\n",
    "pandas >= 1.1.0\n",
    "sklearn >= 0.22.0\n",
    "```\n",
    "\n",
    "### Notes\n",
    "1. Users need to download the datasets from [kaggle](https://www.kaggle.com/datasets) or ```sklearn.datasets```.\n",
    "2. This is an example focused on logistic regression training without high-level data engineering. Thus the LR model result may not be high quality.\n",
    "3. After running this script, make sure to copy the csv files to ```${HE_SAMPLES}/build/examples/logistic-regression/datasets```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import lr_base as lrb\n",
    "import generate_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and preprocessing\n",
    "Download the [Home Credit Default Risk](https://www.kaggle.com/c/home-credit-default-risk) dataset. This example only needs the ```application_train.csv``` file for training, testing and evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update path/to/dataset to the actual path\n",
    "data = pd.read_csv(\"path/to/dataset/application_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding categorical features\n",
    "Convert non-binary categorical features with one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "le_list = []\n",
    "for col in data:\n",
    "  if data[col].dtype == 'object':\n",
    "    if len(list(data[col].unique())) <= 2:\n",
    "      le.fit(data[col])\n",
    "      data[col] = le.transform(data[col])\n",
    "      le_count += 1\n",
    "      le_list.append(col)\n",
    "data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract samples and target\n",
    "samples = data.loc[:, data.columns != 'TARGET']\n",
    "target = data.TARGET\n",
    "\n",
    "feature_columns = data.columns.tolist()\n",
    "feature_columns.remove('TARGET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation transform\n",
    "Fill the missing data points in the sample with ```sklearn.impute.SimpleImputer```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.fillna(samples.median())\n",
    "list_categorical = []\n",
    "for col in samples:\n",
    "  if samples[col].dtype == 'object' and samples[col].isnull().values.any():\n",
    "    list_categorical.append(col)\n",
    "\n",
    "imputer = SimpleImputer()\n",
    "for col in list_categorical:\n",
    "  data = samples[col]\n",
    "  features[col] = imputer.fit_transform(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    samples, target, test_size=0.30, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data points\n",
    "The 3-degree polynomial representation of sigmoid function operates within a limited range, so the sample needs to be scaled to ```[-0.2, 0.2]```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.to_numpy(), X_test.to_numpy()\n",
    "y_train, y_test = y_train.to_numpy(), y_test.to_numpy()\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-.2, .2))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance training set with target\n",
    "The given dataset is highly imbalanced towards ```target = 0```, which can negatively affect the logistic regression training. Thus we adjust the training set to have the target distribution close to 50:50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reallocate 178796 of target 0 from training set to testing set for balanced training\n",
      "After reallocation, 19134 0s and 17327 1s in training set\n"
     ]
    }
   ],
   "source": [
    "target_imba_ratio = 0.99  # imbalance threshold\n",
    "\n",
    "n_y_train_1 = sum(y_train == 1)\n",
    "n_y_train_0 = sum(y_train == 0)\n",
    "n_remove = int((n_y_train_1 - n_y_train_0) * target_imba_ratio)\n",
    "\n",
    "if n_remove > 0:\n",
    "  to_remove = 1\n",
    "else:\n",
    "  to_remove = 0\n",
    "  n_remove *= -1\n",
    "\n",
    "print(\"Reallocate\", n_remove, \"of target\", to_remove,\n",
    "      \"from training set to testing set for balanced training\")\n",
    "\n",
    "ind1 = np.where(y_train == to_remove)[0]\n",
    "ind_remove = np.random.choice(ind1, size=n_remove, replace=False)\n",
    "X_test = np.concatenate([X_test, X_train[ind_remove]], axis=0)\n",
    "y_test = np.concatenate([y_test, y_train[ind_remove]], axis=0)\n",
    "\n",
    "X_train = np.delete(X_train, ind_remove, axis=0)\n",
    "y_train = np.delete(y_train, ind_remove)\n",
    "\n",
    "n_y_train_1 = sum(y_train)\n",
    "n_y_train_0 = y_train.size - n_y_train_1\n",
    "print(\"After reallocation, %s 0s and %s 1s in training set\" %\n",
    "      (n_y_train_0, n_y_train_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test set with test/eval set\n",
    "X_test, X_eval, y_test, y_eval = train_test_split(\n",
    "    X_test, y_test, test_size=0.4, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Training\n",
    "We provide a simple logistic regression training script based on [Homomorphic training of 30,000 logistic regression models](https://eprint.iacr.org/2019/425) by Bergamaschi et al.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "== Logistic Regression Training ==\n",
      "Epoch: 0, - loss: 0.7799580361800673 - acc: 0.972268339174814\n",
      "Epoch: 4, - loss: 2.4352559230475754 - acc: 0.027731660825186005\n",
      "Epoch: 8, - loss: 0.9771821446783528 - acc: 0.027731660825186005\n",
      "Epoch: 12, - loss: 0.7355180931336108 - acc: 0.7264895775687142\n",
      "Epoch: 16, - loss: 0.7278504937344341 - acc: 0.7074709463198672\n",
      "Epoch: 20, - loss: 0.7217525284561388 - acc: 0.6959048146098505\n",
      "Epoch: 24, - loss: 0.7167104285538693 - acc: 0.6899096107729201\n",
      "Epoch: 28, - loss: 0.7123961826994866 - acc: 0.6869089343909488\n",
      "Epoch: 32, - loss: 0.7085991120947206 - acc: 0.6854885322511222\n",
      "Epoch: 36, - loss: 0.7051827498589683 - acc: 0.6852548730246572\n"
     ]
    }
   ],
   "source": [
    "# set verbose = true to view training progress\n",
    "bias, weights = generate_data.doTrain(\n",
    "    X_train, y_train, X_test, y_test, epochs=40, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.6838590665928795   f1 score: 0.09293955753149148\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "_, acc, _, f1 = lrb.test_poly3(X_eval, y_eval,\n",
    "                               np.concatenate(([bias], weights), axis=0))\n",
    "print(\"Accuracy:\", acc, \"  f1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to csv\n",
    "Save the eval set and trained logistic regression model to csv files. The first row of the (eval) data file is the feature names.\n",
    "\n",
    "For using own weights, it must be stored in a single row csv file with ```bias, w[0], w[1], ... , w[n_features - 1]``` template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Eval set size: (108420, 242)\n"
     ]
    }
   ],
   "source": [
    "generate_data.saveModel(\"kaggle_hcdr\", bias, weights)\n",
    "print(\"Eval set size:\", X_eval.shape)\n",
    "generate_data.saveData(\"kaggle_hcdr\", X_eval, y_eval)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
